{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b1afad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 12:49:11.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.tokenizers\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mSpacyTokenizerAdapter : Mod√®le 'en_core_web_sm' charg√© avec succ√®s.\u001b[0m\n",
      "\u001b[32m2026-01-15 12:49:11.694\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmh_nlp.domain.services.text_cleaner\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1mTextCleaner initialis√© avec SpacyTokenizerAdapter (Batch mode: False)\u001b[0m\n",
      "\u001b[32m2026-01-15 12:49:11.694\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmh_nlp.domain.services.text_cleaner\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mTextCleaner initialis√© avec SpacyTokenizerAdapter\u001b[0m\n",
      "\u001b[32m2026-01-15 12:49:11.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.application.use_cases.build_clean_dataset\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mApplication : D√©marrage du Use Case BuildCleanDataset.\u001b[0m\n",
      "\u001b[32m2026-01-15 12:49:11.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.data.kaggle_repository\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mD√©marrage du chargement des donn√©es depuis : ../data/raw/mental_health.csv\u001b[0m\n",
      "\u001b[32m2026-01-15 12:49:12.250\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.infrastructure.data.kaggle_repository\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m57\u001b[0m - \u001b[32m\u001b[1mInfrastructure : 2000 paires (Document, Label) cr√©√©es.\u001b[0m\n",
      "\u001b[32m2026-01-15 12:49:18.548\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.application.use_cases.build_clean_dataset\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m55\u001b[0m - \u001b[32m\u001b[1mApplication : Dataset construit. 1990 documents valides conserv√©s.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "from mh_nlp.domain.services.text_cleaner import TextCleaner\n",
    "from mh_nlp.infrastructure.data.kaggle_repository import KaggleDatasetRepository\n",
    "from mh_nlp.infrastructure.nlp.tokenizers import SpacyTokenizerAdapter\n",
    "from mh_nlp.application.use_cases.build_clean_dataset import BuildCleanDatasetUseCase\n",
    "\n",
    "\n",
    "# 0. D√©finir le mapping des labels\n",
    "label_mapping = {\n",
    "    \"Normal\": 0,\n",
    "    \"Depression\": 1,\n",
    "    \"Suicidal\": 2,\n",
    "    \"Anxiety\": 3,\n",
    "    \"Bipolar\": 4,\n",
    "    \"Stress\": 5,\n",
    "    \"Personality disorder\": 6\n",
    "}\n",
    "\n",
    "filtered_label_mapping = {\n",
    "    \"Anxiety\": 0, \n",
    "    \"Normal\": 1, \n",
    "    \"Depression\": 2,\n",
    "}\n",
    "\n",
    "# 1. Setup des composants (Wiring)\n",
    "repo = KaggleDatasetRepository(\n",
    "    csv_path=\"../data/raw/mental_health.csv\",\n",
    "    label_mapping=filtered_label_mapping,\n",
    ")\n",
    "cleaner = TextCleaner(engine=SpacyTokenizerAdapter(model=\"en_core_web_sm\"))\n",
    "\n",
    "# 2. Initialisation du Use Case\n",
    "use_case = BuildCleanDatasetUseCase(repository=repo, cleaner=cleaner)\n",
    "\n",
    "# 3. Ex√©cution\n",
    "dataset_final = use_case.execute()\n",
    "\n",
    "# Maintenant, dataset_final.texts et dataset_final.labels sont pr√™ts pour Sklearn ou PyTorch !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f56f84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 13:16:31.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.fast_tokenizers\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mSpacyTokenizerAdapter : Mod√®le 'en_core_web_sm' pr√™t (Batch: 10).\u001b[0m\n",
      "\u001b[32m2026-01-15 13:16:31.511\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmh_nlp.domain.services.fast_text_cleaner\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1mFastTextCleaner (Batch Only) pr√™t avec FastSpacyTokenizerAdapter\u001b[0m\n",
      "\u001b[32m2026-01-15 13:16:31.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.fast_tokenizers\u001b[0m:\u001b[36mprocess_batch\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mD√©marrage du traitement par lots (20 lignes).\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- D√©but du Batch Cleaning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Lemmatization (spaCy): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 557.60it/s]\n",
      "\u001b[32m2026-01-15 13:16:31.552\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.fast_tokenizers\u001b[0m:\u001b[36mprocess_batch\u001b[0m:\u001b[36m63\u001b[0m - \u001b[32m\u001b[1mTraitement massif termin√© avec succ√®s.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0 nettoy√© : 'feel depressed lonely today'\n",
      "Doc 1 nettoy√© : ''\n",
      "Doc 2 nettoy√© : ''\n",
      "Doc 3 nettoy√© : 'rt sad'\n",
      "Doc 4 nettoy√© : 'find help'\n",
      "Doc 5 nettoy√© : 'help okay urgent sigh'\n",
      "Doc 6 nettoy√© : 'suffer recommend'\n",
      "Doc 7 nettoy√© : 'anxious stress right'\n",
      "Doc 8 nettoy√© : 'line second line tab'\n",
      "Doc 9 nettoy√© : 'sad'\n",
      "Doc 10 nettoy√© : 'tired'\n",
      "Doc 11 nettoy√© : 'happy today love life'\n",
      "Doc 12 nettoy√© : ''\n",
      "Doc 13 nettoy√© : ''\n",
      "Doc 14 nettoy√© : ''\n",
      "Doc 15 nettoy√© : 't sleep m exhausted shouldn t stay'\n",
      "Doc 16 nettoy√© : 'p b test b html removal p'\n",
      "Doc 17 nettoy√© : 'soooooooo hhaapppyyyyyyy'\n",
      "Doc 18 nettoy√© : 'error user feel low'\n",
      "Doc 19 nettoy√© : 'isthisreal noitisnt helpme'\n"
     ]
    }
   ],
   "source": [
    "from mh_nlp.domain.entities.document import Document\n",
    "from mh_nlp.infrastructure.nlp.fast_tokenizers import FastSpacyTokenizerAdapter\n",
    "from mh_nlp.domain.services.fast_text_cleaner import FastTextCleaner\n",
    "\n",
    "# 1. Initialisation\n",
    "engine = FastSpacyTokenizerAdapter(model=\"en_core_web_sm\", batch_size=10)\n",
    "cleaner = FastTextCleaner(engine=engine)\n",
    "\n",
    "# 2. Donn√©es de test (incluant un cas probl√©matique : le vide)\n",
    "data = [\n",
    "    # 1. Cas Standard\n",
    "    Document(text=\"I feel very depressed and lonely today.\"),\n",
    "    \n",
    "    # 2. Cas Vide (d√©j√† test√©, mais crucial pour l'alignement)\n",
    "    Document(text=\"\"),\n",
    "    \n",
    "    # 3. Espaces uniquement (doit √™tre g√©r√© comme vide)\n",
    "    Document(text=\"     \"),\n",
    "    \n",
    "    # 4. Bruit Social Media massif\n",
    "    Document(text=\"RT @user123: This is so sad!!! #mentalhealth #depression #help @charity_org\"),\n",
    "    \n",
    "    # 5. URLs multiples et complexes\n",
    "    Document(text=\"Found help here: https://support.org/help?id=123 and http://test.com/path\"),\n",
    "    \n",
    "    # 6. Caract√®res sp√©ciaux et ponctuation excessive\n",
    "    Document(text=\"HELP ME !!!... ??? (I am not okay) [urgent] *sigh*\"),\n",
    "    \n",
    "    # 7. Chiffres et dates (doivent √™tre supprim√©s par ta regex [^a-z\\s])\n",
    "    Document(text=\"I have been suffering since 2010. 10/10 would not recommend.\"),\n",
    "    \n",
    "    # 8. M√©lange Majuscules/Minuscules (Case Sensitivity)\n",
    "    Document(text=\"vErY ANXIOUS AND sTrEsSeD out right now.\"),\n",
    "    \n",
    "    # 9. Texte avec sauts de ligne et tabulations\n",
    "    Document(text=\"First line.\\nSecond line with\\ttabs.\"),\n",
    "    \n",
    "    # 10. Texte tr√®s court (un seul mot)\n",
    "    Document(text=\"Sad.\"),\n",
    "    \n",
    "    # 11. Texte tr√®s long (pour tester la performance du batch)\n",
    "    Document(text=\"I am \" + \"so \" * 100 + \"tired.\"),\n",
    "    \n",
    "    # 12. Caract√®res non-ASCII / Emojis (doivent √™tre filtr√©s)\n",
    "    Document(text=\"I am happy today! üòäüåü Love life! ‚ù§Ô∏è\"),\n",
    "    \n",
    "    # 13. Contenu purement num√©rique\n",
    "    Document(text=\"1234567890\"),\n",
    "    \n",
    "    # 14. Contenu purement ponctuation\n",
    "    Document(text=\"!!! ??? @@@ ###\"),\n",
    "    \n",
    "    # 15. Stopwords uniquement (doit retourner une cha√Æne vide apr√®s spaCy)\n",
    "    Document(text=\"I am the and of a\"),\n",
    "    \n",
    "    # 16. Mots avec apostrophes (Contractions)\n",
    "    Document(text=\"I can't sleep, I'm exhausted and shouldn't stay up.\"),\n",
    "    \n",
    "    # 17. Balises HTML (si ton scraping est imparfait)\n",
    "    Document(text=\"<p>This is a <b>test</b> of HTML removal.</p>\"),\n",
    "    \n",
    "    # 18. Texte avec r√©p√©tition de lettres (Slang)\n",
    "    Document(text=\"I am soooooooo hhaapppyyyyyyy\"),\n",
    "    \n",
    "    # 19. Format technique (Logs)\n",
    "    Document(text=\"2023-01-01 12:00:00 [ERROR] User is feeling low\"),\n",
    "    \n",
    "    # 20. Phrases interrogatives et exclamatives coll√©es\n",
    "    Document(text=\"IsThisReal?NoItIsnt!HelpMe.\")\n",
    "]\n",
    "\n",
    "# 3. Test du traitement par lot\n",
    "print(\"--- D√©but du Batch Cleaning ---\")\n",
    "results = cleaner.clean_batch(data)\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"Doc {i} nettoy√© : '{res}'\")\n",
    "\n",
    "# V√©rification de l'alignement\n",
    "assert len(results) == len(data), \"ERREUR : La taille du dataset a chang√© !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f7e074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 13:50:19.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.fast_tokenizers\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mSpacyTokenizerAdapter : Mod√®le 'en_core_web_sm' pr√™t (Batch: 1000).\u001b[0m\n",
      "\u001b[32m2026-01-15 13:50:19.098\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmh_nlp.domain.services.fast_text_cleaner\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1mFastTextCleaner (Batch Only) pr√™t avec FastSpacyTokenizerAdapter\u001b[0m\n",
      "\u001b[32m2026-01-15 13:50:19.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mD√©marrage du pipeline de pr√©paration des donn√©es...\u001b[0m\n",
      "\u001b[32m2026-01-15 13:50:19.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.application.use_cases.build_clean_dataset\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mApplication : Lancement du pipeline de nettoyage de donn√©es.\u001b[0m\n",
      "\u001b[32m2026-01-15 13:50:19.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.data.kaggle_repository\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mD√©marrage du chargement des donn√©es depuis : ../data/raw/mental_health.csv\u001b[0m\n",
      "\u001b[32m2026-01-15 13:50:19.556\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.infrastructure.data.kaggle_repository\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m57\u001b[0m - \u001b[32m\u001b[1mInfrastructure : 2000 paires (Document, Label) cr√©√©es.\u001b[0m\n",
      "\u001b[32m2026-01-15 13:50:19.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.application.use_cases.build_clean_dataset\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mUtilisation du mode Batch (FastTextCleaner)\u001b[0m\n",
      "\u001b[32m2026-01-15 13:50:19.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.fast_tokenizers\u001b[0m:\u001b[36mprocess_batch\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mD√©marrage du traitement par lots (2000 lignes).\u001b[0m\n",
      "Batch Lemmatization (spaCy): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:04<00:00, 462.88it/s]\n",
      "\u001b[32m2026-01-15 13:50:23.920\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.fast_tokenizers\u001b[0m:\u001b[36mprocess_batch\u001b[0m:\u001b[36m63\u001b[0m - \u001b[32m\u001b[1mTraitement massif termin√© avec succ√®s.\u001b[0m\n",
      "\u001b[32m2026-01-15 13:50:23.921\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.application.use_cases.build_clean_dataset\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m85\u001b[0m - \u001b[32m\u001b[1mDataset pr√™t : 1990 documents conserv√©s (10 √©cart√©s car vides).\u001b[0m\n",
      "\u001b[32m2026-01-15 13:50:23.922\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m59\u001b[0m - \u001b[32m\u001b[1mDataset pr√™t pour l'entra√Ænement ! Taille : 1990\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple du premier document : oh gosh...\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "\n",
    "# Infrastructure\n",
    "from mh_nlp.infrastructure.data.kaggle_repository import KaggleDatasetRepository\n",
    "from mh_nlp.infrastructure.nlp.fast_tokenizers import FastSpacyTokenizerAdapter\n",
    "\n",
    "# Domaine (Services)\n",
    "# Note : On utilise FastTextCleaner pour b√©n√©ficier du traitement par lots\n",
    "from mh_nlp.domain.services.fast_text_cleaner import FastTextCleaner\n",
    "\n",
    "# Application (Use Case & DTO)\n",
    "from mh_nlp.application.use_cases.build_clean_dataset import BuildCleanDatasetUseCase\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Mapping des √©tiquettes m√©tier vers les index num√©riques\n",
    "FILTERED_LABEL_MAPPING = {\n",
    "    \"Anxiety\": 0, \n",
    "    \"Normal\": 1, \n",
    "    \"Depression\": 2,\n",
    "}\n",
    "\n",
    "# --- 1. SETUP DES COMPOSANTS (WIRING) ---\n",
    "\n",
    "# Initialisation du d√©p√¥t de donn√©es (Source : CSV Kaggle)\n",
    "repo = KaggleDatasetRepository(\n",
    "    csv_path=\"../data/raw/mental_health.csv\",\n",
    "    label_mapping=FILTERED_LABEL_MAPPING,\n",
    ")\n",
    "\n",
    "# Initialisation du moteur NLP haute performance (Port -> Adapter)\n",
    "# batch_size=1000 permet d'optimiser les performances de spaCy sur CPU\n",
    "spacy_engine = FastSpacyTokenizerAdapter(\n",
    "    model=\"en_core_web_sm\", \n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "# Initialisation du service de nettoyage (Orchestrateur Domaine)\n",
    "cleaner = FastTextCleaner(engine=spacy_engine)\n",
    "\n",
    "# --- 2. INITIALISATION DU USE CASE ---\n",
    "\n",
    "# Le Use Case re√ßoit ses d√©pendances par injection\n",
    "use_case = BuildCleanDatasetUseCase(\n",
    "    repository=repo, \n",
    "    cleaner=cleaner\n",
    ")\n",
    "\n",
    "# --- 3. EX√âCUTION ---\n",
    "\n",
    "logger.info(\"D√©marrage du pipeline de pr√©paration des donn√©es...\")\n",
    "\n",
    "# Lancement du processus complet : Chargement -> Nettoyage Batch -> DTO\n",
    "dataset_final = use_case.execute()\n",
    "\n",
    "# --- 4. V√âRIFICATION ---\n",
    "\n",
    "if dataset_final.total_processed > 0:\n",
    "    logger.success(f\"Dataset pr√™t pour l'entra√Ænement ! Taille : {dataset_final.total_processed}\")\n",
    "    print(f\"Exemple du premier document : {dataset_final.documents[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84a1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc69c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a773d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21396b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3dded0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef2808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 13:19:51.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.fast_tokenizers\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mSpacyTokenizerAdapter : Mod√®le 'en_core_web_sm' pr√™t (Batch: 1000).\u001b[0m\n",
      "\u001b[32m2026-01-15 13:19:51.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.tokenizers\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mSpacyTokenizerAdapter : Mod√®le 'en_core_web_sm' charg√© avec succ√®s.\u001b[0m\n",
      "\u001b[32m2026-01-15 13:19:51.573\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmh_nlp.domain.services.text_cleaner\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1mTextCleaner initialis√© avec SpacyTokenizerAdapter (Batch mode: False)\u001b[0m\n",
      "\u001b[32m2026-01-15 13:19:51.574\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmh_nlp.domain.services.text_cleaner\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mTextCleaner initialis√© avec SpacyTokenizerAdapter\u001b[0m\n",
      "\u001b[32m2026-01-15 13:19:51.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.application.use_cases.build_clean_dataset\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mApplication : D√©marrage du Use Case BuildCleanDataset.\u001b[0m\n",
      "\u001b[32m2026-01-15 13:19:51.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.data.kaggle_repository\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mD√©marrage du chargement des donn√©es depuis : ../data/raw/mental_health.csv\u001b[0m\n",
      "\u001b[32m2026-01-15 13:19:52.031\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.infrastructure.data.kaggle_repository\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m57\u001b[0m - \u001b[32m\u001b[1mInfrastructure : 2000 paires (Document, Label) cr√©√©es.\u001b[0m\n",
      "\u001b[32m2026-01-15 13:19:58.112\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.application.use_cases.build_clean_dataset\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m55\u001b[0m - \u001b[32m\u001b[1mApplication : Dataset construit. 1990 documents valides conserv√©s.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mh_nlp.infrastructure.nlp.fast_tokenizers import FastSpacyTokenizerAdapter\n",
    "\n",
    "spacy_engine = FastSpacyTokenizerAdapter(model=\"en_core_web_sm\",batch_size=1000)\n",
    "print(getattr(spacy_engine, \"batch_size\", None) is not None)\n",
    "\n",
    "filtered_label_mapping = {\n",
    "    \"Anxiety\": 0, \n",
    "    \"Normal\": 1, \n",
    "    \"Depression\": 2,\n",
    "}\n",
    "\n",
    "# 1. Setup des composants (Wiring)\n",
    "repo = KaggleDatasetRepository(\n",
    "    csv_path=\"../data/raw/mental_health.csv\",\n",
    "    label_mapping=filtered_label_mapping,\n",
    ")\n",
    "spacy_engine = FastSpacyTokenizerAdapter(model=\"en_core_web_sm\",batch_size=1000)\n",
    "cleaner = TextCleaner(engine=SpacyTokenizerAdapter(model=\"en_core_web_sm\"))\n",
    "\n",
    "# 2. Initialisation du Use Case\n",
    "use_case = BuildCleanDatasetUseCase(repository=repo, cleaner=cleaner)\n",
    "\n",
    "# 3. Ex√©cution\n",
    "dataset_final = use_case.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05b0a17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-15 11:18:49.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.tokenizers\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSpacyTokenizerAdapter : Mod√®le 'en_core_web_sm' charg√© avec succ√®s.\u001b[0m\n",
      "\u001b[32m2026-01-15 11:18:49.005\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmh_nlp.domain.services.text_cleaner\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m33\u001b[0m - \u001b[34m\u001b[1mTextCleaner initialis√© avec SpacyTokenizerAdapter\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy Result: student study check email\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "from mh_nlp.domain.services.text_cleaner import TextCleaner\n",
    "from mh_nlp.domain.entities.document import Document\n",
    "from mh_nlp.infrastructure.nlp.tokenizers import SpacyTokenizerAdapter, NltkTokenizerAdapter\n",
    "\n",
    "raw_text = \"The students are studying and checking their emails at https://univ.edu !\"\n",
    "document = Document(raw_text)\n",
    "\n",
    "# --- CAS 1 : Utilisation de spaCy (Lemmatisation pr√©cise) ---\n",
    "spacy_engine = SpacyTokenizerAdapter(model=\"en_core_web_sm\")\n",
    "cleaner_spacy = TextCleaner(engine=spacy_engine)\n",
    "print(f\"SpaCy Result: {cleaner_spacy.clean(document)}\")\n",
    "# Sortie attendue: \"student study check email\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CAS 2 : Utilisation de NLTK avec Stemming (Plus agressif) ---\n",
    "nltk_engine = NltkTokenizerAdapter(method=\"stem\")\n",
    "cleaner_nltk = TextCleaner(engine=nltk_engine)\n",
    "print(f\"NLTK Stem Result: {cleaner_nltk.clean(raw_text)}\")\n",
    "# Sortie attendue: \"student studi check email\" (Note le 'studi' tronqu√©)\n",
    "\n",
    "# --- CAS 3 : Utilisation de NLTK avec Lemmatisation ---\n",
    "nltk_lem = NltkTokenizerAdapter(method=\"lemmatize\")\n",
    "cleaner_lem = TextCleaner(engine=nltk_lem)\n",
    "print(f\"NLTK Lemma Result: {cleaner_lem.clean(raw_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f98516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mh_nlp.infrastructure.data.kaggle_repository import KaggleDatasetRepository\n",
    "\n",
    "# 1. D√©finir le mapping des labels (nom -> index num√©rique)\n",
    "label_mapping = {\n",
    "    \"Normal\": 0,\n",
    "    \"Depression\": 1,\n",
    "    \"Suicidal\": 2,\n",
    "    \"Anxiety\": 3,\n",
    "    \"Bipolar\": 4,\n",
    "    \"Stress\": 5,\n",
    "    \"Personality disorder\": 6\n",
    "}\n",
    "\n",
    "# 2. Charger les donn√©es\n",
    "repo = KaggleDatasetRepository(\n",
    "    csv_path=\"../data/raw/mental_health.csv\",\n",
    "    label_mapping=label_mapping\n",
    ")\n",
    "raw_data = repo.load()\n",
    "\n",
    "# 3. Transformer en dataset exploitable\n",
    "dataset = []\n",
    "for document, label in raw_data:\n",
    "    dataset.append({\n",
    "        'text': document.text,\n",
    "        'label': label.name,\n",
    "        'label_id': label.index\n",
    "    })\n",
    "\n",
    "# Afficher quelques exemples\n",
    "print(f\"Nombre total d'exemples : {len(dataset)}\")\n",
    "print(\"\\nPremiers exemples :\")\n",
    "for i, item in enumerate(dataset[:3]):\n",
    "    print(f\"\\nExemple {i+1}:\")\n",
    "    print(f\"  Texte: {item['text'][:100]}...\")\n",
    "    print(f\"  Label: {item['label']} (ID: {item['label_id']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative : format simplifi√©\n",
    "dataset = [\n",
    "    (document.text, label.index) \n",
    "    for document, label in raw_data\n",
    "]\n",
    "\n",
    "# Exemple d'utilisation\n",
    "texts, labels = zip(*dataset)\n",
    "print(f\"Nombre de textes : {len(texts)}\")\n",
    "print(f\"Premier texte : {texts[0]}\")\n",
    "print(f\"Premier label : {labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f9d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©parer textes et labels\n",
    "texts = [document.text for document, label in raw_data]\n",
    "labels = [label.index for document, label in raw_data]\n",
    "label_names = [label.name for document, label in raw_data]\n",
    "\n",
    "print(f\"Nombre d'exemples : {len(texts)}\")\n",
    "print(f\"\\nExemple 1:\")\n",
    "print(f\"  Texte: {texts[0]}\")\n",
    "print(f\"  Label: {label_names[0]} ({labels[0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1e18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b117810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement du test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage spaCy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 94.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORIGINAL : I am running in the park and I love it! #happy\n",
      "NETTOY√â  : run park love\n",
      "\n",
      "ORIGINAL : Check out this website: https://openai.com for more info.\n",
      "NETTOY√â  : check website info\n",
      "\n",
      "ORIGINAL : The cats are sitting on the mats. @someone\n",
      "NETTOY√â  : cat sit mat\n",
      "\n",
      "ORIGINAL : Better late than never, but never late is better.\n",
      "NETTOY√â  : well late late well\n",
      "\n",
      "ORIGINAL : I've been studying NLP with spaCy for 3 hours!\n",
      "NETTOY√â  : have study nlp spacy hour\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Chargement optimis√© du mod√®le\n",
    "# On d√©sactive le 'parser' (syntaxe) et le 'ner' (entit√©s nomm√©es) pour gagner en vitesse\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def fast_clean_spacy(texts):\n",
    "    cleaned_texts = []\n",
    "    \n",
    "    # Pr√©traitement l√©ger par RegEx avant d'envoyer √† spaCy\n",
    "    # On le fait sous forme de g√©n√©rateur pour √©conomiser la m√©moire\n",
    "    def pre_process(t_list):\n",
    "        for text in t_list:\n",
    "            text = text.lower()\n",
    "            text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "            text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "            text = re.sub(r'[^a-z\\s]', '', text)\n",
    "            yield text\n",
    "\n",
    "    # 2. Utilisation de nlp.pipe avec tqdm pour la barre de progression\n",
    "    # nlp.pipe est beaucoup plus rapide que .apply() car il traite par lots (batches)\n",
    "    for doc in tqdm(nlp.pipe(pre_process(texts), batch_size=500), \n",
    "                    total=len(texts), \n",
    "                    desc=\"Nettoyage spaCy\"):\n",
    "        \n",
    "        # Lemmatisation et retrait des stopwords / espaces\n",
    "        words = [token.lemma_ for token in doc if not token.is_stop and not token.is_space]\n",
    "        cleaned_texts.append(' '.join(words))\n",
    "        \n",
    "    return cleaned_texts\n",
    "\n",
    "# --- Application au DataFrame ---\n",
    "# --- Donn√©es de test ---\n",
    "test_sentences = [\n",
    "    \"I am running in the park and I love it! #happy\", \n",
    "    \"Check out this website: https://openai.com for more info.\",\n",
    "    \"The cats are sitting on the mats. @someone\",\n",
    "    \"Better late than never, but never late is better.\",\n",
    "    \"I've been studying NLP with spaCy for 3 hours!\"\n",
    "]\n",
    "\n",
    "# --- Ex√©cution du test ---\n",
    "print(\"Lancement du test...\")\n",
    "cleaned_results = fast_clean_spacy(test_sentences)\n",
    "\n",
    "# --- Affichage comparatif ---\n",
    "for original, cleaned in zip(test_sentences, cleaned_results):\n",
    "    print(f\"\\nORIGINAL : {original}\")\n",
    "    print(f\"NETTOY√â  : {cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1fcb75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement de 100 lignes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nettoyage spaCy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 336.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aper√ßu des r√©sultats (5 premiers) ---\n",
      "Ligne 1 : website sit amazing cat\n",
      "Ligne 2 : run park data dog\n",
      "Ligne 3 : learn spacy machine learning nltk\n",
      "Ligne 4 : have cat well hour fast\n",
      "Ligne 5 : learn spacy machine learning nltk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# --- G√©n√©rateur de 100 lignes de test ---\n",
    "templates = [\n",
    "    \"I am running in the park with my {} dogs! #nature\",\n",
    "    \"Check this link {} for more details on {} @user\",\n",
    "    \"The {} is sitting on the {}, it's so {}!\",\n",
    "    \"Learning {} with spaCy is {} than NLTK.\",\n",
    "    \"I've been {} for {} hours now, it's {}.\"\n",
    "]\n",
    "\n",
    "words = [\"cat\", \"data\", \"better\", \"running\", \"machine learning\", \"fast\", \"website\", \"amazing\"]\n",
    "urls = [\"http://test.com\", \"https://example.org\", \"www.nlp-cool.io\"]\n",
    "\n",
    "# Cr√©ation de la liste de 100 phrases\n",
    "test_sentences_100 = []\n",
    "for i in range(100):\n",
    "    tpl = random.choice(templates)\n",
    "    # On remplit les trous {} avec des mots al√©atoires pour varier\n",
    "    if tpl.count(\"{}\") == 1:\n",
    "        sentence = tpl.format(random.choice(words))\n",
    "    elif tpl.count(\"{}\") == 2:\n",
    "        sentence = tpl.format(random.choice(urls), random.choice(words))\n",
    "    else:\n",
    "        sentence = tpl.format(random.choice(words), random.choice(words), random.choice(words))\n",
    "    test_sentences_100.append(sentence)\n",
    "\n",
    "# --- Ex√©cution avec la fonction fast_clean_spacy ---\n",
    "print(f\"Traitement de {len(test_sentences_100)} lignes...\")\n",
    "results = fast_clean_spacy(test_sentences_100)\n",
    "\n",
    "# Affichage des 5 premiers r√©sultats pour v√©rification\n",
    "print(\"\\n--- Aper√ßu des r√©sultats (5 premiers) ---\")\n",
    "for i in range(5):\n",
    "    print(f\"Ligne {i+1} : {results[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27535d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset pr√™t : 5000 lignes.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Construction d'un dataset h√©t√©rog√®ne\n",
    "raw_texts = [\n",
    "    \"The AI revolution is transforming software engineering globally.\", # Court\n",
    "    \"Natural Language Processing (NLP) allows machines to understand human speech.\" * 20, # Long/R√©p√©titif\n",
    "    \"I'm running, he runs, she ran; we've been running for miles in the rain.\", # Conjugaisons (Test Lemme)\n",
    "    \"Contact us at support@example.com or visit https://nlp.spacy.io for docs.\", # Entit√©s/URL\n",
    "    \"This is a very simple sentence.\" # Tr√®s court\n",
    "]\n",
    "\n",
    "# On multiplie pour atteindre 5000 lignes (suffisant pour voir la diff√©rence de perf)\n",
    "dataset = raw_texts * 1000 \n",
    "print(f\"Dataset pr√™t : {len(dataset)} lignes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "# Simulation d'un dataset de test (remplacez par vos donn√©es r√©elles)\n",
    "test_data = [\n",
    "    \"The quick brown fox jumps over the lazy dog and looks for more adventure.\" * 10\n",
    "] * 1000  # 1000 documents de taille moyenne\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)  # Retourne en Mo\n",
    "\n",
    "def run_benchmark(adapter, data: List[str], name: str):\n",
    "    print(f\"--- Benchmarking: {name} ---\")\n",
    "    \n",
    "    start_mem = get_memory_usage()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Ex√©cution du traitement par lots\n",
    "    results = adapter.process_batch(data)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_mem = get_memory_usage()\n",
    "    \n",
    "    duration = end_time - start_time\n",
    "    mem_diff = end_mem - start_mem\n",
    "    throughput = len(data) / duration\n",
    "    \n",
    "    return {\n",
    "        \"Mod√®le\": name,\n",
    "        \"Temps total (s)\": round(duration, 3),\n",
    "        \"Docs/seconde\": round(throughput, 2),\n",
    "        \"M√©moire utilis√©e (Mo)\": round(mem_diff, 2)\n",
    "    }\n",
    "\n",
    "# 1. Initialisation des adaptateurs\n",
    "spacy_adapter = SpacyTokenizerAdapter(model=\"en_core_web_sm\", batch_size=1000)\n",
    "#nltk_adapter = NltkTokenizerAdapter(method=\"lemmatize\")\n",
    "\n",
    "# 2. Lancement des tests\n",
    "results = []\n",
    "results.append(run_benchmark(spacy_adapter, test_data, \"spaCy (Optimized)\"))\n",
    "results.append(run_benchmark(nltk_adapter, test_data, \"NLTK (WordNet)\"))\n",
    "\n",
    "# 3. Affichage des r√©sultats\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nTableau Comparatif :\")\n",
    "display(df_results)\n",
    "\n",
    "# 4. Visualisation\n",
    "df_results.set_index(\"Mod√®le\")[[\"Temps total (s)\", \"Docs/seconde\"]].plot(\n",
    "    kind=\"bar\", subplots=True, figsize=(10, 6), title=\"Performance Comparison\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b5ee7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset pr√™t : 5000 lignes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mode: Unitaire Full: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:32<00:00, 54.30it/s]\n",
      "Mode: Batch Full: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:11<00:00, 70.14it/s]\n",
      "Mode: Batch Optimized: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:33<00:00, 149.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Configuration</th>\n",
       "      <th>Temps (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unitaire (Full Pipeline)</td>\n",
       "      <td>92.085369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Batch (Full Pipeline)</td>\n",
       "      <td>71.288010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Batch (Optimized: No NER/Parser)</td>\n",
       "      <td>33.527106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Configuration  Temps (s)\n",
       "0          Unitaire (Full Pipeline)  92.085369\n",
       "1             Batch (Full Pipeline)  71.288010\n",
       "2  Batch (Optimized: No NER/Parser)  33.527106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Construction d'un dataset h√©t√©rog√®ne\n",
    "raw_texts = [\n",
    "    \"The AI revolution is transforming software engineering globally.\", # Court\n",
    "    \"Natural Language Processing (NLP) allows machines to understand human speech.\" * 20, # Long/R√©p√©titif\n",
    "    \"I'm running, he runs, she ran; we've been running for miles in the rain.\", # Conjugaisons (Test Lemme)\n",
    "    \"Contact us at support@example.com or visit https://nlp.spacy.io for docs.\", # Entit√©s/URL\n",
    "    \"This is a very simple sentence.\" # Tr√®s court\n",
    "]\n",
    "\n",
    "# On multiplie pour atteindre 5000 lignes (suffisant pour voir la diff√©rence de perf)\n",
    "dataset = raw_texts * 1000 \n",
    "print(f\"Dataset pr√™t : {len(dataset)} lignes.\")\n",
    "\n",
    "import time\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def benchmark_spacy_configs(texts):\n",
    "    results = []\n",
    "    \n",
    "    # --- CONFIG 1 : NO OPTIMIZED (Unitaire + Tous composants) ---\n",
    "    nlp_full = spacy.load(\"en_core_web_sm\")\n",
    "    start = time.time()\n",
    "    res_1 = [ [t.lemma_ for t in nlp_full(txt) if not t.is_stop] for txt in tqdm(texts, desc=\"Mode: Unitaire Full\") ]\n",
    "    results.append({\"Configuration\": \"Unitaire (Full Pipeline)\", \"Temps (s)\": time.time() - start})\n",
    "\n",
    "    # --- CONFIG 2 : SEMI OPTIMIZED (Batch + Tous composants) ---\n",
    "    start = time.time()\n",
    "    res_2 = []\n",
    "    for doc in tqdm(nlp_full.pipe(texts, batch_size=1000), total=len(texts), desc=\"Mode: Batch Full\"):\n",
    "        res_2.append([t.lemma_ for t in doc if not t.is_stop])\n",
    "    results.append({\"Configuration\": \"Batch (Full Pipeline)\", \"Temps (s)\": time.time() - start})\n",
    "\n",
    "    # --- CONFIG 3 : FULL OPTIMIZED (Batch + Disabled Components) ---\n",
    "    # C'est ce que fait votre classe SpacyTokenizerAdapter\n",
    "    nlp_fast = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "    start = time.time()\n",
    "    res_3 = []\n",
    "    for doc in tqdm(nlp_fast.pipe(texts, batch_size=1000), total=len(texts), desc=\"Mode: Batch Optimized\"):\n",
    "        res_3.append([t.lemma_ for t in doc if not t.is_stop])\n",
    "    results.append({\"Configuration\": \"Batch (Optimized: No NER/Parser)\", \"Temps (s)\": time.time() - start})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Ex√©cution\n",
    "df_bench = benchmark_spacy_configs(dataset)\n",
    "display(df_bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a044b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mh-nlp-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
