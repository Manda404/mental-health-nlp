{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91e5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Supprime les avertissements de d√©pr√©ciation de numpy/tensorflow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Configuration de l'affichage pour Notebook\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6dd815d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 01:39:49.952\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmh_nlp.domain.services.text_cleaner\u001b[0m:\u001b[36mclean\u001b[0m:\u001b[36m38\u001b[0m - \u001b[33m\u001b[1mDocument vide ou nul re√ßu ‚Üí nettoyage ignor√©.\u001b[0m\n",
      "\u001b[32m2026-01-17 01:39:49.954\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmh_nlp.domain.services.text_cleaner\u001b[0m:\u001b[36mclean\u001b[0m:\u001b[36m38\u001b[0m - \u001b[33m\u001b[1mDocument vide ou nul re√ßu ‚Üí nettoyage ignor√©.\u001b[0m\n",
      "\u001b[32m2026-01-17 01:39:49.954\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmh_nlp.domain.services.text_cleaner\u001b[0m:\u001b[36mclean\u001b[0m:\u001b[36m38\u001b[0m - \u001b[33m\u001b[1mDocument vide ou nul re√ßu ‚Üí nettoyage ignor√©.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyse de 25 documents termin√©e.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Cleaned</th>\n",
       "      <th>Length_Before</th>\n",
       "      <th>Length_After</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am NOT feeling good today... üòû</td>\n",
       "      <td>am not feeling good today</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I feel anxious and I can't sleep.</td>\n",
       "      <td>feel anxious and can't sleep</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am feeling very happy and energetic!</td>\n",
       "      <td>am feeling very happy and energetic</td>\n",
       "      <td>38</td>\n",
       "      <td>35</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I feel very depressed and lonely today.</td>\n",
       "      <td>feel very depressed and lonely today</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vErY ANXIOUS AND sTrEsSeD out right now.</td>\n",
       "      <td>very anxious and stressed out right now</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sad.</td>\n",
       "      <td>sad</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Check this out: https://help.com/support @user123 #mentalhealth</td>\n",
       "      <td>check this out</td>\n",
       "      <td>63</td>\n",
       "      <td>14</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @user123: This is so sad!!! #mentalhealth #depression #help @charity_org</td>\n",
       "      <td>rt this is so sad</td>\n",
       "      <td>75</td>\n",
       "      <td>17</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Found help here: https://support.org/help?id=123 and http://test.com/path</td>\n",
       "      <td>found help here and</td>\n",
       "      <td>73</td>\n",
       "      <td>19</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;p&gt;This is a &lt;b&gt;test&lt;/b&gt; of HTML removal.&lt;/p&gt;</td>\n",
       "      <td>this is test of html removal</td>\n",
       "      <td>45</td>\n",
       "      <td>28</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HELP ME !!!... ??? (I am not okay) [urgent] *sigh*</td>\n",
       "      <td>help me am not okay urgent sigh</td>\n",
       "      <td>50</td>\n",
       "      <td>31</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>!!! ??? @@@ ###</td>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I am happy today! üòäüåü Love life! ‚ù§Ô∏è</td>\n",
       "      <td>am happy today love life</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>IsThisReal?NoItIsnt!HelpMe.</td>\n",
       "      <td>isthisreal noitisnt helpme</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I have been suffering since 2010. 10/10 would not recommend.</td>\n",
       "      <td>have been suffering since would not recommend</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>First line.\\nSecond line with\\ttabs.</td>\n",
       "      <td>first line second line with tabs</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1234567890</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-01-01 12:00:00 [ERROR] User is feeling low</td>\n",
       "      <td>error user is feeling low</td>\n",
       "      <td>47</td>\n",
       "      <td>25</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>/ restlessness appears. Thanks!</td>\n",
       "      <td>restlessness appears thanks</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I am so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so...</td>\n",
       "      <td>am so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so tired</td>\n",
       "      <td>161</td>\n",
       "      <td>158</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I can't sleep, I'm exhausted and shouldn't stay up.</td>\n",
       "      <td>can't sleep i'm exhausted and shouldn't stay up</td>\n",
       "      <td>51</td>\n",
       "      <td>47</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I am soooooooo hhaapppyyyyyyy</td>\n",
       "      <td>am soooooooo hhaapppyyyyyyy</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   Original  \\\n",
       "0                                                                          I am NOT feeling good today... üòû   \n",
       "1                                                                         I feel anxious and I can't sleep.   \n",
       "2                                                                    I am feeling very happy and energetic!   \n",
       "3                                                                   I feel very depressed and lonely today.   \n",
       "4                                                                  vErY ANXIOUS AND sTrEsSeD out right now.   \n",
       "5                                                                                                      Sad.   \n",
       "6                                           Check this out: https://help.com/support @user123 #mentalhealth   \n",
       "7                               RT @user123: This is so sad!!! #mentalhealth #depression #help @charity_org   \n",
       "8                                 Found help here: https://support.org/help?id=123 and http://test.com/path   \n",
       "9                                                             <p>This is a <b>test</b> of HTML removal.</p>   \n",
       "10                                                       HELP ME !!!... ??? (I am not okay) [urgent] *sigh*   \n",
       "11                                                                                          !!! ??? @@@ ###   \n",
       "12                                                                       I am happy today! üòäüåü Love life! ‚ù§Ô∏è   \n",
       "13                                                                              IsThisReal?NoItIsnt!HelpMe.   \n",
       "14                                             I have been suffering since 2010. 10/10 would not recommend.   \n",
       "15                                                                     First line.\\nSecond line with\\ttabs.   \n",
       "16                                                                                               1234567890   \n",
       "17                                                          2023-01-01 12:00:00 [ERROR] User is feeling low   \n",
       "18                                                                                                            \n",
       "19                                                                                                            \n",
       "20                                                                                                            \n",
       "21                                                                          / restlessness appears. Thanks!   \n",
       "22  I am so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so...   \n",
       "23                                                      I can't sleep, I'm exhausted and shouldn't stay up.   \n",
       "24                                                                            I am soooooooo hhaapppyyyyyyy   \n",
       "\n",
       "                                                                                                                                                           Cleaned  \\\n",
       "0                                                                                                                                        am not feeling good today   \n",
       "1                                                                                                                                     feel anxious and can't sleep   \n",
       "2                                                                                                                              am feeling very happy and energetic   \n",
       "3                                                                                                                             feel very depressed and lonely today   \n",
       "4                                                                                                                          very anxious and stressed out right now   \n",
       "5                                                                                                                                                              sad   \n",
       "6                                                                                                                                                   check this out   \n",
       "7                                                                                                                                                rt this is so sad   \n",
       "8                                                                                                                                              found help here and   \n",
       "9                                                                                                                                     this is test of html removal   \n",
       "10                                                                                                                                 help me am not okay urgent sigh   \n",
       "11                                                                                                                                                                   \n",
       "12                                                                                                                                        am happy today love life   \n",
       "13                                                                                                                                      isthisreal noitisnt helpme   \n",
       "14                                                                                                                   have been suffering since would not recommend   \n",
       "15                                                                                                                                first line second line with tabs   \n",
       "16                                                                                                                                                                   \n",
       "17                                                                                                                                       error user is feeling low   \n",
       "18                                                                                                                                                                   \n",
       "19                                                                                                                                                                   \n",
       "20                                                                                                                                                                   \n",
       "21                                                                                                                                     restlessness appears thanks   \n",
       "22  am so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so tired   \n",
       "23                                                                                                                 can't sleep i'm exhausted and shouldn't stay up   \n",
       "24                                                                                                                                     am soooooooo hhaapppyyyyyyy   \n",
       "\n",
       "    Length_Before  Length_After Status  \n",
       "0              32            25     OK  \n",
       "1              33            28     OK  \n",
       "2              38            35     OK  \n",
       "3              39            36     OK  \n",
       "4              40            39     OK  \n",
       "5               4             3     OK  \n",
       "6              63            14     OK  \n",
       "7              75            17     OK  \n",
       "8              73            19     OK  \n",
       "9              45            28     OK  \n",
       "10             50            31     OK  \n",
       "11             15             0  Empty  \n",
       "12             34            24     OK  \n",
       "13             27            26     OK  \n",
       "14             60            45     OK  \n",
       "15             34            32     OK  \n",
       "16             10             0  Empty  \n",
       "17             47            25     OK  \n",
       "18              4             0  Empty  \n",
       "19              1             0  Empty  \n",
       "20              0             0  Empty  \n",
       "21             31            27     OK  \n",
       "22            161           158     OK  \n",
       "23             51            47     OK  \n",
       "24             29            27     OK  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from mh_nlp.domain.entities.document import Document\n",
    "from mh_nlp.domain.services.text_cleaner import TextCleaner\n",
    "\n",
    "# ==========================================\n",
    "# 1. INITIALISATION DU SERVICE\n",
    "# ==========================================\n",
    "# Instanciation du cleaner (Logique m√©tier du domaine)\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "# ==========================================\n",
    "# 2. PR√âPARATION DU JEU DE TEST (DATASET)\n",
    "# ==========================================\n",
    "# Liste exhaustive couvrant les cas limites (Edge Cases)\n",
    "data = [\n",
    "    # --- Groupe A : Cas Standards & √âmotions ---\n",
    "    Document(text=\"I am NOT feeling good today... üòû\"),\n",
    "    Document(text=\"I feel anxious and I can't sleep.\"),\n",
    "    Document(text=\"I am feeling very happy and energetic!\"),\n",
    "    Document(text=\"I feel very depressed and lonely today.\"),\n",
    "    Document(text=\"vErY ANXIOUS AND sTrEsSeD out right now.\"),\n",
    "    Document(text=\"Sad.\"),\n",
    "    \n",
    "    # --- Groupe B : Bruit Social Media & Web ---\n",
    "    Document(text=\"Check this out: https://help.com/support @user123 #mentalhealth\"),\n",
    "    Document(text=\"RT @user123: This is so sad!!! #mentalhealth #depression #help @charity_org\"),\n",
    "    Document(text=\"Found help here: https://support.org/help?id=123 and http://test.com/path\"),\n",
    "    Document(text=\"<p>This is a <b>test</b> of HTML removal.</p>\"),\n",
    "    \n",
    "    # --- Groupe C : Ponctuation & Caract√®res Sp√©ciaux ---\n",
    "    Document(text=\"HELP ME !!!... ??? (I am not okay) [urgent] *sigh*\"),\n",
    "    Document(text=\"!!! ??? @@@ ###\"),\n",
    "    Document(text=\"I am happy today! üòäüåü Love life! ‚ù§Ô∏è\"),\n",
    "    Document(text=\"IsThisReal?NoItIsnt!HelpMe.\"),\n",
    "    \n",
    "    # --- Groupe D : Formats Techniques & Nombres ---\n",
    "    Document(text=\"I have been suffering since 2010. 10/10 would not recommend.\"),\n",
    "    Document(text=\"First line.\\nSecond line with\\ttabs.\"),\n",
    "    Document(text=\"1234567890\"),\n",
    "    Document(text=\"2023-01-01 12:00:00 [ERROR] User is feeling low\"),\n",
    "    \n",
    "    # --- Groupe E : Cas \"Vides\" ou Probl√©matiques ---\n",
    "    Document(text=\"    \"), \n",
    "    Document(text=\" \"),\n",
    "    Document(text=\"\"),\n",
    "    Document(text=\"/ restlessness appears. Thanks!\"),\n",
    "    Document(text=\"I am \" + \"so \" * 50 + \"tired.\"), # Test de performance/longueur\n",
    "    \n",
    "    # --- Groupe F : Linguistique (Apostrophes & Slang) ---\n",
    "    Document(text=\"I can't sleep, I'm exhausted and shouldn't stay up.\"),\n",
    "    Document(text=\"I am soooooooo hhaapppyyyyyyy\")\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# 3. EX√âCUTION DU PIPELINE DE NETTOYAGE\n",
    "# ==========================================\n",
    "results = []\n",
    "\n",
    "for doc in data:\n",
    "    # Appel de la m√©thode clean du domaine\n",
    "    cleaned_text = cleaner.clean(doc)\n",
    "    \n",
    "    results.append({\n",
    "        \"Original\": doc.text.strip()[:100] + \"...\" if len(doc.text) > 100 else doc.text,\n",
    "        \"Cleaned\": cleaned_text,\n",
    "        \"Length_Before\": len(doc.text),\n",
    "        \"Length_After\": len(cleaned_text),\n",
    "        \"Status\": \"OK\" if cleaned_text else \"Empty\"\n",
    "    })\n",
    "\n",
    "# ==========================================\n",
    "# 4. VISUALISATION ET ANALYSE DES R√âSULTATS\n",
    "# ==========================================\n",
    "df_cleaned = pd.DataFrame(results)\n",
    "\n",
    "print(f\"Analyse de {len(data)} documents termin√©e.\\n\")\n",
    "display(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b70743c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 01:39:50.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.spacy_tokenizer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mInitialisation de SpacyTokenizer : en_core_web_sm\u001b[0m\n",
      "\u001b[32m2026-01-17 01:39:50.810\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.spacy_tokenizer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m43\u001b[0m - \u001b[32m\u001b[1mMod√®le spaCy 'en_core_web_sm' pr√™t.\u001b[0m\n",
      "\u001b[32m2026-01-17 01:39:50.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.spacy_tokenizer\u001b[0m:\u001b[36mtokenize\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mTokenisation de 25 documents (Batch: 10)\u001b[0m\n",
      "NLP Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<00:00, 700.87doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text nettoy√© original: am not feeling good today\n",
      "Sortie spaCy (Lemmes): ['feel', 'good', 'today']\n",
      "------\n",
      "Text nettoy√© original: feel anxious and can't sleep\n",
      "Sortie spaCy (Lemmes): ['feel', 'anxious', 'sleep']\n",
      "------\n",
      "Text nettoy√© original: am feeling very happy and energetic\n",
      "Sortie spaCy (Lemmes): ['feel', 'happy', 'energetic']\n",
      "------\n",
      "Text nettoy√© original: feel very depressed and lonely today\n",
      "Sortie spaCy (Lemmes): ['feel', 'depressed', 'lonely', 'today']\n",
      "------\n",
      "Text nettoy√© original: very anxious and stressed out right now\n",
      "Sortie spaCy (Lemmes): ['anxious', 'stress', 'right']\n",
      "------\n",
      "Text nettoy√© original: sad\n",
      "Sortie spaCy (Lemmes): ['sad']\n",
      "------\n",
      "Text nettoy√© original: check this out\n",
      "Sortie spaCy (Lemmes): ['check']\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mh_nlp.infrastructure.nlp.spacy_tokenizer import SpacyTokenizer\n",
    "\n",
    "documents = [Document(text) for text in df_cleaned.Cleaned]\n",
    "\n",
    "spacy_tok = SpacyTokenizer(model_name=\"en_core_web_sm\", lemmatize=True, remove_stop= True, batch_size=10)\n",
    "spacy_output = spacy_tok.tokenize(documents)\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    print(f\"Text nettoy√© original: {documents[i].text}\")\n",
    "    print(f\"Sortie spaCy (Lemmes): {spacy_output[i]}\")\n",
    "    print(\"--\"*3)\n",
    "\n",
    "    if i > 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "744b38cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 01:39:51.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.spacy_tokenizer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mInitialisation de SpacyTokenizer : en_core_web_sm\u001b[0m\n",
      "\u001b[32m2026-01-17 01:39:51.329\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.spacy_tokenizer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m43\u001b[0m - \u001b[32m\u001b[1mMod√®le spaCy 'en_core_web_sm' pr√™t.\u001b[0m\n",
      "\u001b[32m2026-01-17 01:39:51.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.spacy_tokenizer\u001b[0m:\u001b[36mtokenize\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mTokenisation de 25 documents (Batch: 10)\u001b[0m\n",
      "NLP Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<00:00, 1289.60doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text nettoy√© original: am not feeling good today\n",
      "Sortie spaCy (Lemmes): ['feel', 'good', 'today']\n",
      "------\n",
      "Text nettoy√© original: feel anxious and can't sleep\n",
      "Sortie spaCy (Lemmes): ['feel', 'anxious', 'sleep']\n",
      "------\n",
      "Text nettoy√© original: am feeling very happy and energetic\n",
      "Sortie spaCy (Lemmes): ['feel', 'happy', 'energetic']\n",
      "------\n",
      "Text nettoy√© original: feel very depressed and lonely today\n",
      "Sortie spaCy (Lemmes): ['feel', 'depressed', 'lonely', 'today']\n",
      "------\n",
      "Text nettoy√© original: very anxious and stressed out right now\n",
      "Sortie spaCy (Lemmes): ['anxious', 'stress', 'right']\n",
      "------\n",
      "Text nettoy√© original: sad\n",
      "Sortie spaCy (Lemmes): ['sad']\n",
      "------\n",
      "Text nettoy√© original: check this out\n",
      "Sortie spaCy (Lemmes): ['check']\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mh_nlp.infrastructure.nlp.spacy_tokenizer import SpacyTokenizer\n",
    "\n",
    "documents = [Document(text) for text in df_cleaned.Cleaned]\n",
    "\n",
    "spacy_tok = SpacyTokenizer(model_name=\"en_core_web_sm\", lemmatize=True, batch_size=10)\n",
    "spacy_output = spacy_tok.tokenize(documents)\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    print(f\"Text nettoy√© original: {documents[i].text}\")\n",
    "    print(f\"Sortie spaCy (Lemmes): {spacy_output[i]}\")\n",
    "    print(\"--\"*3)\n",
    "\n",
    "    if i > 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98bb74d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 01:40:48.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.application.use_cases.build_dataset\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mD√©marrage du pipeline BuildCleanDataset.\u001b[0m\n",
      "\u001b[32m2026-01-17 01:40:48.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.data.kaggle_repository\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mD√©marrage du chargement des donn√©es depuis : ../data/raw/mental_health.csv\u001b[0m\n",
      "\u001b[32m2026-01-17 01:40:48.299\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.infrastructure.data.kaggle_repository\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m57\u001b[0m - \u001b[32m\u001b[1mInfrastructure : 1000 paires (Document, Label) cr√©√©es.\u001b[0m\n",
      "\u001b[32m2026-01-17 01:40:48.302\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmh_nlp.application.use_cases.build_dataset\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1mTraitement unitaire de 1000 documents.\u001b[0m\n",
      "\u001b[32m2026-01-17 01:40:48.337\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.application.use_cases.build_dataset\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m92\u001b[0m - \u001b[32m\u001b[1mPipeline termin√© : 1000 entit√©s synchronis√©es. (0 supprim√©es car vides).\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble sleeping confused mind restless heart all out of tune</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all wrong back off dear forward doubt stay in restless and restless place</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                cleaned text  \\\n",
       "0                                                                 oh my gosh   \n",
       "1              trouble sleeping confused mind restless heart all out of tune   \n",
       "2  all wrong back off dear forward doubt stay in restless and restless place   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from loguru import logger\n",
    "from mh_nlp.domain.services.text_cleaner import TextCleaner\n",
    "from mh_nlp.infrastructure.data.kaggle_repository import KaggleDatasetRepository\n",
    "from mh_nlp.application.use_cases.build_dataset import BuildCleanDatasetUseCase\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "label_mapping = {\n",
    "    \"Anxiety\": 0, \n",
    "    \"Normal\": 1, \n",
    "    \"Depression\": 2,\n",
    "}\n",
    "\n",
    "# --- 2. INSTANCIATION DES COMPOSANTS ---\n",
    "# Initialisation de l'acc√®s aux donn√©es et du service de nettoyage\n",
    "repository = KaggleDatasetRepository(\n",
    "    csv_path=\"../data/raw/mental_health.csv\",\n",
    "    label_mapping=label_mapping,\n",
    ")\n",
    "\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "# --- 3. ORCHESTRATION ---\n",
    "# Injection des d√©pendances dans le cas d'usage\n",
    "use_case = BuildCleanDatasetUseCase(\n",
    "    repository=repository, \n",
    "    cleaner=cleaner\n",
    ")\n",
    "\n",
    "# --- 4. EX√âCUTION ---\n",
    "# Lancement du pipeline et r√©cup√©ration du DatasetDTO\n",
    "dataset_final = use_case.execute()\n",
    "\n",
    "# --- 5. Extraction des donn√©es (List comprehension pour plus d'efficacit√©)\n",
    "texts = [doc.text for doc in dataset_final.documents]\n",
    "labels = [label for label in dataset_final.labels]\n",
    "\n",
    "# Cr√©ation du DataFrame\n",
    "# Utiliser un dictionnaire est la m√©thode la plus propre\n",
    "dataset = pd.DataFrame({\n",
    "    'cleaned text': texts,\n",
    "    'label': labels,\n",
    "})\n",
    "\n",
    "# Afficher les premi√®res lignes pour v√©rifier\n",
    "display(dataset.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182558e7",
   "metadata": {},
   "source": [
    "# Split Clened Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e4c336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 01:43:44.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.application.use_cases.split_dataset\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mD√©coupage du dataset (Total: 1000) | Cible: Test=0.2, Val=0.1\u001b[0m\n",
      "\u001b[32m2026-01-17 01:43:44.841\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.application.use_cases.split_dataset\u001b[0m:\u001b[36mexecute\u001b[0m:\u001b[36m86\u001b[0m - \u001b[32m\u001b[1mSplit r√©ussi : Train=700 | Val=100 | Test=200\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille Train : 700\n",
      "Premier document Train : anxiety fear overthinking at the same time can be called panic attack right\n",
      "Premier label Train : 0\n"
     ]
    }
   ],
   "source": [
    "from mh_nlp.application.use_cases.split_dataset import SplitDatasetUseCase\n",
    "\n",
    "# 1. Instanciation du Use Case avec vos ratios\n",
    "# Ici: 70% Train / 10% Val / 20% Test\n",
    "splitter = SplitDatasetUseCase(test_size=0.2, val_size=0.1)\n",
    "\n",
    "# 2. Ex√©cution du split\n",
    "splits = splitter.execute(dataset_final)\n",
    "\n",
    "# 3. R√©cup√©ration des r√©sultats\n",
    "train_set = splits[\"train\"]\n",
    "val_set = splits[\"val\"]\n",
    "test_set = splits[\"test\"]\n",
    "\n",
    "print(f\"Taille Train : {train_set.total_records}\")\n",
    "print(f\"Premier document Train : {train_set.documents[0].text}\")\n",
    "print(f\"Premier label Train : {train_set.labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0d09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80d32d56",
   "metadata": {},
   "source": [
    "# Apply Tokenizer fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "375fd486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mh_nlp.domain.entities.document import Document\n",
    "\n",
    "# Jeu de test minimaliste\n",
    "raw_texts = [\n",
    "    \"I am feeling very anxious today and I can't sleep.\",\n",
    "    \"The therapy sessions are helping me manage my stress better.\",\n",
    "    \"I feel much better after talking to my friend.\"\n",
    "]\n",
    "\n",
    "# Conversion en entit√©s du domaine\n",
    "documents = [Document(text) for text in raw_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad1b1b4",
   "metadata": {},
   "source": [
    "# A. Le Tokenizer spaCy (Linguistique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ce3d891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 13:59:13.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.spacy_tokenizer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mInitialisation de SpacyTokenizer : en_core_web_sm\u001b[0m\n",
      "\u001b[32m2026-01-17 13:59:15.213\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.spacy_tokenizer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m43\u001b[0m - \u001b[32m\u001b[1mMod√®le spaCy 'en_core_web_sm' pr√™t.\u001b[0m\n",
      "\u001b[32m2026-01-17 13:59:15.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.spacy_tokenizer\u001b[0m:\u001b[36mtokenize\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mTokenisation de 3 documents (Batch: 2)\u001b[0m\n",
      "NLP Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 28.39doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sortie spaCy (Lemmes) ---\n",
      "['feel', 'anxious', 'today', 'sleep']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mh_nlp.infrastructure.nlp.spacy_tokenizer import SpacyTokenizer\n",
    "\n",
    "spacy_tok = SpacyTokenizer(model_name=\"en_core_web_sm\", lemmatize=True, batch_size=2)\n",
    "spacy_output = spacy_tok.tokenize(documents)\n",
    "\n",
    "print(\"--- Sortie spaCy (Lemmes) ---\")\n",
    "print(spacy_output[0]) \n",
    "# Devrait afficher: ['feel', 'anxious', 'today', 'sleep']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dd01f",
   "metadata": {},
   "source": [
    "# B. Le Tokenizer HuggingFace (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9277a0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 13:59:22.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.hf_tokenizer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mChargement du tokenizer HF : distilbert-base-uncased (max_length=16)\u001b[0m\n",
      "\u001b[32m2026-01-17 13:59:23.703\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.hf_tokenizer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m36\u001b[0m - \u001b[32m\u001b[1mTokenizer distilbert-base-uncased charg√© avec succ√®s.\u001b[0m\n",
      "\u001b[32m2026-01-17 13:59:23.708\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.hf_tokenizer\u001b[0m:\u001b[36mtokenize\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1mTokenisation en cours pour 3 documents (Mod√®le: distilbert-base-uncased)\u001b[0m\n",
      "\u001b[32m2026-01-17 13:59:23.740\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.hf_tokenizer\u001b[0m:\u001b[36mtokenize\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1mTokenisation termin√©e. Forme du tenseur : [3, 15]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sortie HuggingFace (Tenseurs) ---\n",
      "tensor([  101,  1045,  2572,  3110,  2200, 11480,  2651,  1998,  1045,  2064,\n",
      "         1005,  1056,  3637,  1012,   102])\n",
      "Shape: torch.Size([3, 15])\n"
     ]
    }
   ],
   "source": [
    "from mh_nlp.infrastructure.nlp.hf_tokenizer import HuggingFaceTokenizer\n",
    "\n",
    "hf_tok = HuggingFaceTokenizer(model_name=\"distilbert-base-uncased\", max_length=16)\n",
    "hf_output = hf_tok.tokenize(documents)\n",
    "\n",
    "print(\"\\n--- Sortie HuggingFace (Tenseurs) ---\")\n",
    "print(hf_output['input_ids'][0]) # Affiche les IDs num√©riques\n",
    "print(f\"Shape: {hf_output['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17bf1b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1045,  2572,  3110,  2200, 11480,  2651,  1998,  1045,  2064,\n",
       "          1005,  1056,  3637,  1012,   102],\n",
       "        [  101,  1996,  7242,  6521,  2024,  5094,  2033,  6133,  2026,  6911,\n",
       "          2488,  1012,   102,     0,     0],\n",
       "        [  101,  1045,  2514,  2172,  2488,  2044,  3331,  2000,  2026,  2767,\n",
       "          1012,   102,     0,     0,     0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_output['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da3829",
   "metadata": {},
   "source": [
    "# C. Le Tokenizer Keras (CNN/LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34451b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-16 11:34:06.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.keras_tokenizer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mKerasTextTokenizer initialis√© (Vocab target: 1000, Max length: 16)\u001b[0m\n",
      "\u001b[32m2026-01-16 11:34:06.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.keras_tokenizer\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mApprentissage du vocabulaire sur 3 documents...\u001b[0m\n",
      "\u001b[32m2026-01-16 11:34:06.082\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.keras_tokenizer\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m52\u001b[0m - \u001b[32m\u001b[1mVocabulaire appris : 26 mots uniques trouv√©s.\u001b[0m\n",
      "\u001b[32m2026-01-16 11:34:06.093\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmh_nlp.infrastructure.nlp.keras_tokenizer\u001b[0m:\u001b[36mtokenize\u001b[0m:\u001b[36m82\u001b[0m - \u001b[34m\u001b[1mS√©quences g√©n√©r√©es : (3, 16) (Type: int32)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sortie Keras (Numpy Matrix) ---\n",
      "[ 2  5  6  7  8  9 10  2 11 12  0  0  0  0  0  0]\n",
      "Shape: (3, 16)\n"
     ]
    }
   ],
   "source": [
    "from mh_nlp.infrastructure.nlp.keras_tokenizer import KerasTextTokenizer\n",
    "\n",
    "keras_tok = KerasTextTokenizer(vocab_size=1000, max_length=16)\n",
    "keras_tok.fit(documents) # Obligatoire pour Keras\n",
    "keras_output = keras_tok.tokenize(documents)\n",
    "\n",
    "print(\"\\n--- Sortie Keras (Numpy Matrix) ---\")\n",
    "print(keras_output[0])\n",
    "print(f\"Shape: {keras_output.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mh-nlp-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
